{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rai-sandeep/ai-colab-notebooks/blob/main/f7b-trained-v100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzGa3YSCgl4C"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7wgbsa4U2TBk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77010933-57e1-46b4-e75c-38b21f0f12b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: HF_TOKEN=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\n"
          ]
        }
      ],
      "source": [
        "# https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "model_id = \"vilsonrodrigues/falcon-7b-sharded\"\n",
        "trained_model_id = \"falcon-7b-sharded-template\"\n",
        "\n",
        "#ds_id = \"rai-sandeep/dataset_template\"\n",
        "\n",
        "#ds_id = \"rai-sandeep/wp_combined_dataset\"\n",
        "\n",
        "#example_ques = \"Generate a White Paper ABSTRACT on Modernizing Enterprise Systems.\"\n",
        "#example_ques2 = \"Generate a White Paper MAINCONTENT on Bitcoin Mining.\"\n",
        "max_tok = 1000\n",
        "tempre = 0.1\n",
        "\n",
        "save_trained_model=False\n",
        "%env HF_TOKEN=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"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WGeTgx1Hgljb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e389c5c-63c1-4518-81be-048eddcb11b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U einops\n",
        "!pip install -q -U safetensors\n",
        "!pip install -q -U torch\n",
        "!pip install -q -U xformers\n",
        "!pip install -q -U datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1zodOgcgqnO"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "v_r4SL_KmHGs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d1eb87b-9ffa-4047-94d8-64e1bd5b89b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('//172.28.0.1'), PosixPath('8013')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-28liqjc8yxsvc --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Quantized Model"
      ],
      "metadata": {
        "id": "alLhGWu9Ngvg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tz4eBuxZHi7F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "5863d04c9fc244f88ef04093b39dbb0a",
            "08dae82b3af34b50808efc41f3c79bf9",
            "0c2e048ab8ee4ae49a51e58342c408e0",
            "3012aac53128465aa45178d1d98f550e",
            "9dcf127eb3ab4f559cd11df134a62b3f",
            "5e8f0d44ca024a68b118febfd0b226d6",
            "a11ec74626a54366875d323862a84511",
            "2b839cb56aad4831bb9cb361d9cd9318",
            "55e2c70983654cd6b25e1763d9e0c592",
            "8e9b32bb00b740c4931908ae0e86d06e",
            "97a3b08215dc453092e97cf20c83055f"
          ]
        },
        "outputId": "412fc401-c0e6-4747-8384-0d5f4583c7f8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5863d04c9fc244f88ef04093b39dbb0a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}, trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "Hd22_v_mkbZy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "HOhd6empkd0_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"query_key_value\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "-qjN7qOkkgKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee8f8a9c-f6db-4094-807b-d5d612c4c6ae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 4718592 || all params: 3613463424 || trainable%: 0.13058363808693696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Data"
      ],
      "metadata": {
        "id": "mxDuoyQvQfD2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UZbrRk4J3qVH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "20e94c50eba844c2bb11d2a9207df60d",
            "a054702c8e36425e8de9e6cfe273b741",
            "41ddb55bf09741a1bf889597f360bab7",
            "52f211861691495fa93b8ce321824d71",
            "cb58e3c6aca74ce1bece740aea07baf0",
            "790d618d6f494c92aed2c17b746b53f0",
            "0a6ee891bcc945f7bcd5a4b35d12a97c",
            "11046050edcf496db5f331b32c971bfc",
            "d2164264c3244832823c5e1c5b4bb00a",
            "d0dc60f294ca4766a8fccc2a041e4d7a",
            "e295a57e0068434f84432faffc94e0b5"
          ]
        },
        "outputId": "f1f84c34-65ee-4257-d198-2d65b40b23da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-c95349fce4b52d96/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20e94c50eba844c2bb11d2a9207df60d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43\n",
            "['This white paper examines the role of artificial intelligence (AI) in modernizing enterprise systems. It provides insights into how AI can transform traditional manual testing processes into automated validation models that accelerate time to market. The paper discusses the existing challenges of traditional testing automation and highlights how AI and machine learning (ML) can resolve these challenges. By exploring five use-cases and solutions, it demonstrates how AI/ML can enable complete and intelligent automation with minimal human intervention, empowering testing teams to become truly agile. The findings presented in this paper contribute to the ongoing conversation surrounding the digitization and automation of enterprise systems, offering practical recommendations for organizations aiming to modernize their testing approaches through the adoption of AI technologies.']\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "#data = load_dataset(ds_id)\n",
        "data = load_dataset(\"csv\", data_files=\"/content/dataset_content.csv\")\n",
        "\n",
        "ds_len=len(data['train'])\n",
        "print(ds_len)\n",
        "rows_sel=data['train'].select(range(1))\n",
        "print(rows_sel['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "c7Pbb_6H6C2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21134aa5-934a-4fdd-a4e7-fb86d42660ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c95349fce4b52d96/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-2ec18635b98a3923.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'doctype': 'White Paper', 'section': 'Abstract', 'contenttype': 'example', 'topic': 'Role of Artificial Intelligence (AI) in Modernizing Enterprise Systems', 'content': 'This white paper examines the role of artificial intelligence (AI) in modernizing enterprise systems. It provides insights into how AI can transform traditional manual testing processes into automated validation models that accelerate time to market. The paper discusses the existing challenges of traditional testing automation and highlights how AI and machine learning (ML) can resolve these challenges. By exploring five use-cases and solutions, it demonstrates how AI/ML can enable complete and intelligent automation with minimal human intervention, empowering testing teams to become truly agile. The findings presented in this paper contribute to the ongoing conversation surrounding the digitization and automation of enterprise systems, offering practical recommendations for organizations aiming to modernize their testing approaches through the adoption of AI technologies.', 'input_text': '>>QUESTION<<Generate Abstract section for a White Paper on topic Role of Artificial Intelligence (AI) in Modernizing Enterprise Systems\\n>>ANSWER<<This white paper examines the role of artificial intelligence (AI) in modernizing enterprise systems. It provides insights into how AI can transform traditional manual testing processes into automated validation models that accelerate time to market. The paper discusses the existing challenges of traditional testing automation and highlights how AI and machine learning (ML) can resolve these challenges. By exploring five use-cases and solutions, it demonstrates how AI/ML can enable complete and intelligent automation with minimal human intervention, empowering testing teams to become truly agile. The findings presented in this paper contribute to the ongoing conversation surrounding the digitization and automation of enterprise systems, offering practical recommendations for organizations aiming to modernize their testing approaches through the adoption of AI technologies.<|endoftext|>'}\n"
          ]
        }
      ],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# prompt_template = \"### Instruction: {prompt}\\n### Response:\"\n",
        "\n",
        "#train_dataset = data['train'].select(range(ds_len)).map(lambda x: {\"input_text\": \"Template for \" + x['doctype'] + \" \" + x['section'] + \":\\n\" + x['template']})\n",
        "\n",
        "#train_dataset = data['train'].select(range(ds_len)).map(lambda x: {\"input_text\": \"Use this \" + x['contenttype'] + \" to generate \" + x['section'] + \" for a \" + x['doctype'] + \": \" + x['content']})\n",
        "\n",
        "train_dataset = data['train'].select(range(ds_len)).map(lambda x: {\"input_text\": \">>QUESTION<<Generate \" + x['section'] + \" section for a \" + x['doctype'] + \" on topic \" + x['topic'] + \"\\n>>ANSWER<<\" + x['content'] + \"<|endoftext|>\"})\n",
        "\n",
        "print(train_dataset[0])\n",
        "# Tokenize the datasets\n",
        "train_encodings = tokenizer(train_dataset['input_text'], max_length=256, truncation=True, padding=True, return_tensors='pt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  class TextDataset(Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = item[\"input_ids\"].clone()\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n"
      ],
      "metadata": {
        "id": "whNBQA9m0EPb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the encodings to PyTorch datasets\n",
        "train_dataset = TextDataset(train_encodings)"
      ],
      "metadata": {
        "id": "hrNxx33K0H9p"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "J53zPp1qSdZp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDniJ-bq4nxD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "8ff81709-4029-4d01-f7fd-a85df17b0f7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-bd500a371ea7>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12/50 03:14 < 12:18, 0.05 it/s, Epoch 7/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.747700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.577700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.166800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.132900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.605400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.679900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.654400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.548400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.042400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.023300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    # eval_dataset=val_dataset,\n",
        "    args=transformers.TrainingArguments(\n",
        "        num_train_epochs=50,\n",
        "        per_device_train_batch_size=16,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_ratio=0.05,\n",
        "        # max_steps=100,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        lr_scheduler_type='cosine',\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "\n",
        "#import gc\n",
        "#del train_encodings, data\n",
        "#gc.collect()\n",
        "#torch.cuda.empty_cache()\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example After Fine Tuning"
      ],
      "metadata": {
        "id": "KWXT-FNfSlpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.use_cache = True\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "_9vrd1FXSz8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(doc_type, topic, keywords, content_to_use):\n",
        "\n",
        "  #content_to_use = \"INFOSYS WATCH TOWER is AN AI PLATFORM FOR COMPREHENSIVE AIRPORT SECURITY. Infosys Watch Tower is an advanced AI platform that provides comprehensive security solutions for airports. It offers real-time monitoring capabilities to address the specific challenges faced by airports, such as crowd management and ensuring a secure environment. By integrating various sensors, including cameras, drones, and LiDAR, the platform detects incidents and gathers valuable information to streamline airport operations.With its cutting-edge technologies and scalability, Infosys Watch Tower enables efficient tracking of cargo and mitigates security threats effectively. The platform utilizes drones for inspecting aircraft and employs Non-destructive Testing (NDT) techniques using sonar to identify material damage. It leverages 5G technology to facilitate the creation of low-code/no-code AI workflows for installed sensors, enabling seamless configuration and deployment across multiple edges.Infosys Watch Tower offers a unified view of inferences, reports, monitoring plans, and alerts, providing management with a 360-degree visibility into different incidents and events. It serves as a valuable tool for security personnel, operations managers, and other stakeholders by enhancing their decision-making capabilities and ensuring the overall security and maintenance of the airport.The platform encompasses various use cases crucial to the aviation industry. It includes fire and smoke detection, enabling prompt identification and alerting security personnel. Bird monitoring is another significant aspect, as bird strikes pose a high risk to aircraft safety. By capturing images of birds flying over airfields, Infosys Watch Tower can notify authorities and trigger measures to divert the birds' path, preventing potential collisions.Furthermore, the platform facilitates unattended cabin baggage detection, effectively tracking baggage and identifying any unattended or abandoned luggage. It also offers features for perimeter monitoring, construction monitoring, and vegetation monitoring, addressing key concerns related to security breaches, construction safety, and wildlife interference.Infosys Watch Tower ensures a higher level of security, safety, and operational efficiency for airports, making it an indispensable solution in the aviation industry.\"\n",
        "\n",
        "  doc_content=\"\"\n",
        "  doc_parts=[\"Abstract\",\"Problem Statement\",\"Infosys Solution\",\"Conclusion\"]\n",
        "  max_words=[100,100]\n",
        "  #doc_parts=[\"Abstract\",\"Introduction\",\"Problem Statement\",\"Methodology\",\"Conclusion\"]\n",
        "\n",
        "  for doc_part in doc_parts:\n",
        "    prompt = \"\"\n",
        "\n",
        "    #prompt0 = f\"Generate {doc_part} section for a {doc_type}.\\n\"\n",
        "    prompt1 = \"\"\n",
        "    if content_to_use.strip() != \"\":\n",
        "      prompt1 = f\"Here is some information about {topic} for your reference: {content_to_use}\\n\"\n",
        "\n",
        "    prompt2 = \"\"\n",
        "    if keywords.strip() != \"\":\n",
        "      prompt2= f\"Include the following keywords in your response: {keywords}.\\n\"\n",
        "\n",
        "    #prompt3=f\">>TITLE<<{doc_type} on {topic}\\n\"\n",
        "    prompt4 = f\"Generate {doc_part} section for a {doc_type} on topic {topic}\\n\"\n",
        "    #prompt5=f\"## {doc_part}\\n\"\n",
        "\n",
        "    #prompt3 = f\"\\n>>QUESTION<<{questn}\\n>>ANSWER<<\"\n",
        "    #data['train']\n",
        "\n",
        "    prompt_parts = [prompt1, prompt2, prompt4]\n",
        "\n",
        "    for prompt_part in prompt_parts:\n",
        "      if prompt_part.strip() != \"\":\n",
        "        prompt += prompt_part\n",
        "\n",
        "    print(f\"\\n-------------\\nPrompt:\\n{prompt}\\n-------------\\n\")\n",
        "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "\n",
        "    output = model.generate(\n",
        "        input_ids=encoding.input_ids,\n",
        "        attention_mask=encoding.attention_mask,\n",
        "        max_new_tokens=200,\n",
        "        max_time=600,\n",
        "        do_sample=True,\n",
        "        temperature=0.1,\n",
        "        repetition_penalty=10.0,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        top_k = 0\n",
        "    )\n",
        "\n",
        "    temp_content=tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    #prompt_parts = [prompt0, prompt1, prompt2, prompt3.replace(\">>TITLE<<\",\"\"), prompt4]\n",
        "    for prompt_part in prompt_parts:\n",
        "      if prompt_part.strip() != \"\":\n",
        "        temp_content=temp_content.replace(prompt_part, \"\")\n",
        "\n",
        "    temp_content=f\"\\n## {doc_part}\\n\\n{temp_content}\\n\\n\"\n",
        "    print(temp_content)\n",
        "    doc_content+=temp_content\n",
        "\n",
        "  #print(doc_content)\n",
        "\n",
        "  f = open(topic.replace(\" \", \"-\")+\"-\"+\".md\", \"w\")\n",
        "  f.write(doc_content)\n",
        "  f.close()"
      ],
      "metadata": {
        "id": "xx4mKCzFPEM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"White Paper\", \"Infosys Watch Tower\", \"\", \"\")"
      ],
      "metadata": {
        "id": "MezO1msTSnEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_for_input=False\n",
        "\n",
        "if prompt_for_input:\n",
        "  doc_type = input(\"What type of document do you want to generate? (Enter White Paper or Press Release)\\n\")\n",
        "  topic = input(f\"On what topic do you want to generate the {doc_type}?\\n\")\n",
        "  keywords = input(f\"Enter any keywords that should be part of the {doc_type} in comma separated format. If none, press Enter.\\n\")\n",
        "  content_to_use = input(f\"Enter any content that should be used for the {doc_type}. If none, press Enter.\\n\")\n",
        "\n",
        "generate(doc_type, topic, keywords, content_to_use)"
      ],
      "metadata": {
        "id": "1KVZmmld3g4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the Model"
      ],
      "metadata": {
        "id": "w60z4hyJzdaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_trained_model = False\n",
        "\n",
        "if (save_trained_model):\n",
        "  model.save_pretrained(\"model-falcon-trained\")\n",
        "  !huggingface-cli login --token $HF_TOKEN\n",
        "  model.push_to_hub(\"rai-sandeep/model-falcon-trained\")"
      ],
      "metadata": {
        "id": "SDRHSSnZznmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8bK8VfO33_WI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_and_generate():\n",
        "\n",
        "  doc_type = input(\"What type of document do you want to generate? (Enter White Paper or Press Release)\\n\")\n",
        "  topic = input(f\"On what topic do you want to generate the {doc_type}?\\n\")\n",
        "  keywords = input(f\"Enter any keywords that should be part of the {doc_type} in comma separated format. If none, press Enter.\\n\")\n",
        "  content_to_use = input(f\"Enter any content that should be used for the {doc_type}. If none, press Enter.\\n\")\n",
        "\n",
        "  doc_content=\"\"\n",
        "  doc_parts=[\"Abstract\",\"Introduction\",\"Problem Statement\",\"Methodology\",\"Findings and Analysis\",\"Recommendations\",\"Conclusion\"]\n",
        "\n",
        "  for doc_part in doc_parts:\n",
        "    prompt = f\"Generate a {doc_type} {doc_part} on {topic}.\"\n",
        "    if keywords.strip() != \"\":\n",
        "      prompt += f\"\\nInclude the following keywords: {keywords}.\"\n",
        "    if content_to_use.strip() != \"\":\n",
        "      prompt += f\"\\nHere is some context about {topic}: {content_to_use}.\\n\"\n",
        "\n",
        "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "    output = model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=max_tok, do_sample=True, temperature=tempre, eos_token_id=tokenizer.eos_token_id, top_k = 0)\n",
        "\n",
        "    temp_content=tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    temp_content=temp_content.replace(prompt, \"\")\n",
        "    doc_content+=f\"\\n## {doc_part}  \\n{temp_content}  \\n\"\n",
        "\n",
        "  print(doc_content)\n",
        "\n",
        "  f = open(topic.replace(\" \", \"-\")+\".md\", \"w\")\n",
        "  f.write(doc_content)\n",
        "  f.close()"
      ],
      "metadata": {
        "id": "rFjXfYivrnGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt_and_generate()"
      ],
      "metadata": {
        "id": "79gmk3y1v6WK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5863d04c9fc244f88ef04093b39dbb0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_08dae82b3af34b50808efc41f3c79bf9",
              "IPY_MODEL_0c2e048ab8ee4ae49a51e58342c408e0",
              "IPY_MODEL_3012aac53128465aa45178d1d98f550e"
            ],
            "layout": "IPY_MODEL_9dcf127eb3ab4f559cd11df134a62b3f"
          }
        },
        "08dae82b3af34b50808efc41f3c79bf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e8f0d44ca024a68b118febfd0b226d6",
            "placeholder": "​",
            "style": "IPY_MODEL_a11ec74626a54366875d323862a84511",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0c2e048ab8ee4ae49a51e58342c408e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b839cb56aad4831bb9cb361d9cd9318",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_55e2c70983654cd6b25e1763d9e0c592",
            "value": 15
          }
        },
        "3012aac53128465aa45178d1d98f550e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e9b32bb00b740c4931908ae0e86d06e",
            "placeholder": "​",
            "style": "IPY_MODEL_97a3b08215dc453092e97cf20c83055f",
            "value": " 15/15 [02:24&lt;00:00,  7.83s/it]"
          }
        },
        "9dcf127eb3ab4f559cd11df134a62b3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e8f0d44ca024a68b118febfd0b226d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a11ec74626a54366875d323862a84511": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b839cb56aad4831bb9cb361d9cd9318": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55e2c70983654cd6b25e1763d9e0c592": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e9b32bb00b740c4931908ae0e86d06e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97a3b08215dc453092e97cf20c83055f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20e94c50eba844c2bb11d2a9207df60d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a054702c8e36425e8de9e6cfe273b741",
              "IPY_MODEL_41ddb55bf09741a1bf889597f360bab7",
              "IPY_MODEL_52f211861691495fa93b8ce321824d71"
            ],
            "layout": "IPY_MODEL_cb58e3c6aca74ce1bece740aea07baf0"
          }
        },
        "a054702c8e36425e8de9e6cfe273b741": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_790d618d6f494c92aed2c17b746b53f0",
            "placeholder": "​",
            "style": "IPY_MODEL_0a6ee891bcc945f7bcd5a4b35d12a97c",
            "value": "100%"
          }
        },
        "41ddb55bf09741a1bf889597f360bab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11046050edcf496db5f331b32c971bfc",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d2164264c3244832823c5e1c5b4bb00a",
            "value": 1
          }
        },
        "52f211861691495fa93b8ce321824d71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0dc60f294ca4766a8fccc2a041e4d7a",
            "placeholder": "​",
            "style": "IPY_MODEL_e295a57e0068434f84432faffc94e0b5",
            "value": " 1/1 [00:00&lt;00:00, 34.31it/s]"
          }
        },
        "cb58e3c6aca74ce1bece740aea07baf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "790d618d6f494c92aed2c17b746b53f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a6ee891bcc945f7bcd5a4b35d12a97c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11046050edcf496db5f331b32c971bfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2164264c3244832823c5e1c5b4bb00a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d0dc60f294ca4766a8fccc2a041e4d7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e295a57e0068434f84432faffc94e0b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}